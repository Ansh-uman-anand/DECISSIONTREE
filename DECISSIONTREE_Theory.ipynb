{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **THEORY**"
      ],
      "metadata": {
        "id": "plgkp7jOIM6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.  What is a Decision Tree, and how does it work?**"
      ],
      "metadata": {
        "id": "srw9yP9rIdeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree is a supervised learning algorithm used for classification and regression tasks.\n",
        "It splits data into branches based on feature conditions, forming a tree-like structure where each internal node represents a decision rule,\n",
        "each branch represents an outcome, and each leaf node represents a final class or value."
      ],
      "metadata": {
        "id": "JQMA1mJHIk2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.  What are impurity measures in Decision Trees ?**"
      ],
      "metadata": {
        "id": "OYMOp2oqI7xZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impurity measures quantify the disorder in a dataset. Common impurity measures include:\n",
        "\n",
        "**Gini Impurity** (used in CART)\n",
        "\n",
        "**Entropy** (used in ID3 and C4.5)\n",
        "\n",
        "**Variance Reduction** (used in regression trees)"
      ],
      "metadata": {
        "id": "XQbnTpddJYd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.  What is the mathematical formula for Gini Impurity?**"
      ],
      "metadata": {
        "id": "vNaIksYZJrbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini Impurity measures how often a randomly chosen element from the dataset would be incorrectly labeled if it were randomly classified according to the distribution of labels in the dataset.\n",
        "\n",
        " The formula for Gini Impurity is:\n",
        "\n",
        "Gini = 1 - (Sum of squared probabilities of each class)\n",
        "\n",
        "For example, if a node contains 70% of class A and 30% of class B, the Gini Impurity would be:\n",
        "\n",
        "Gini = 1 - (0.7² + 0.3²) = 1 - (0.49 + 0.09) = 0.42"
      ],
      "metadata": {
        "id": "ABwOfEcmJwWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.  What is the mathematical formula for Entropy?**"
      ],
      "metadata": {
        "id": "s_a4rRTzKDvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy measures the randomness or unpredictability in a dataset. It is used to determine the best attribute for splitting in a decision tree.\n",
        "\n",
        "The formula for Entropy is:\n",
        "\n",
        "Entropy = - (Sum of (probability of each class * log2(probability of each class)))\n",
        "\n",
        "For example, if a node contains 70% of class A and 30% of class B, the Entropy would be:\n",
        "\n",
        "Entropy = - (0.7 * log2(0.7) + 0.3 * log2(0.3)) ≈ 0.88.\n",
        "\n",
        "A lower entropy value means a purer node, which helps in making better splits."
      ],
      "metadata": {
        "id": "IS8VxoW7KI5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What is Information Gain, and how is it used in Decision Trees?**"
      ],
      "metadata": {
        "id": "plHTJWe3KdF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain (IG) is a measure of how much uncertainty (entropy) is reduced after a dataset is split on a feature.\n",
        "\n",
        "The formula for Information Gain is:\n",
        "\n",
        " IG = Entropy(parent) - Weighted sum of Entropy(children)\n",
        "\n",
        " For example, if a dataset is split into two subsets where the uncertainty is significantly reduced, the Information Gain will be high.\n",
        "\n",
        " A higher Information Gain indicates a better feature for splitting the data, helping to create a more effective Decision Tree.)\n",
        "\n",
        " A higher Information Gain means a better split."
      ],
      "metadata": {
        "id": "W88dvOYtKdjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What is the difference between Gini Impurity and Entropy?**"
      ],
      "metadata": {
        "id": "k8puC7rHKt9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Gini Impurity** is computationally simpler and tends to create purer nodes quickly.  \n",
        "**Entropy** considers the probability distribution more precisely but is computationally expensive."
      ],
      "metadata": {
        "id": "4JN8L0exKx28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.  What is the mathematical explanation behind Decision Trees?**"
      ],
      "metadata": {
        "id": "AlzTbBcuK5Ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A Decision Tree recursively splits data based on impurity reduction.\n",
        " It selects features using a criterion like Gini Impurity or Entropy and applies recursive partitioning until a stopping condition (e.g., max depth) is met."
      ],
      "metadata": {
        "id": "Us_EnXoqK8s5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.  What is Pre-Pruning in Decision Trees?**"
      ],
      "metadata": {
        "id": "8sU5RHxhLFHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Pruning stops tree growth early using conditions like max depth, min samples per leaf, or min impurity decrease to prevent overfitting."
      ],
      "metadata": {
        "id": "rGL_T8wiLItq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.  What is Post-Pruning in Decision Trees?**"
      ],
      "metadata": {
        "id": "TFBITTOxLL4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-Pruning removes unnecessary branches after the tree is fully grown, usually based on validation data."
      ],
      "metadata": {
        "id": "ZhJ4vik_LUKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. What is the difference between Pre-Pruning and Post-Pruning?**"
      ],
      "metadata": {
        "id": "xWytOlVWLW_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pre-Pruning prevents tree overgrowth.  \n",
        "- Post-Pruning removes unnecessary complexity after the tree is built."
      ],
      "metadata": {
        "id": "hP-7OXkaLa9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. What is a Decision Tree Regressor?**"
      ],
      "metadata": {
        "id": "JqrcGmklLhln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree Regressor predicts continuous values by minimizing variance at each split."
      ],
      "metadata": {
        "id": "mFyAtCqsLiIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. What are the advantages and disadvantages of Decision Trees?**"
      ],
      "metadata": {
        "id": "D9WeMXLpLpXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages:**  \n",
        "  - Easy to interpret  \n",
        "  - Handles categorical and numerical data  \n",
        "  - Requires little data preprocessing  \n",
        "\n",
        "**Disadvantages:**  \n",
        "  - Prone to overfitting  \n",
        "  - Unstable with small changes in data  \n",
        "  - Biased towards dominant classes if not balanced  "
      ],
      "metadata": {
        "id": "v3fICs9DLs6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. How does a Decision Tree handle missing values?**"
      ],
      "metadata": {
        "id": "wNFRpSjiL6Es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It can ignore missing values while splitting.  \n",
        "- It can use surrogate splits (alternative features).  \n",
        "- It can use imputation methods (mean, median, mode).  "
      ],
      "metadata": {
        "id": "nAnlQa3SL63y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. How does a Decision Tree handle categorical features?**"
      ],
      "metadata": {
        "id": "FpiZrhjrMGVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It uses one-hot encoding or label encoding.  \n",
        "- Some implementations split based on category frequency."
      ],
      "metadata": {
        "id": "H2fbyaaoMGtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. What are some real-world applications of Decision Trees?**"
      ],
      "metadata": {
        "id": "PPfcLIBxMTqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Medical diagnosis** (e.g., disease prediction)  \n",
        "**Finance** (e.g., credit risk assessment)  \n",
        "**Marketing** (e.g., customer segmentation)  \n",
        "**Fraud detection**  "
      ],
      "metadata": {
        "id": "emvXmN_fMXuj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFJfdzT8ILfn"
      },
      "outputs": [],
      "source": []
    }
  ]
}